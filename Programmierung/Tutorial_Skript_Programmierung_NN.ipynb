{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start\n",
    "\n",
    "Dieses Skript führt Sie Schritt für Schritt durch die Programmierung Neuronaler Netze in Python/Tensorflow. \n",
    "\n",
    "Über den Pfeil neben der Abschnittsnummer können die einzelnen Code-Abschnitte ein- und ausgeklappt werden. Manche Kapitel beinhalten weitere Unterabschnitte.\n",
    "Über den \"Ausführen\"/ \"Execute Cell\"-Button links neben einem Code-Block wird der entsprechende Code-Abschnitt ausgeführt. Das Programm-Feedback (prints) werden unter dem zugehörigen Code-Abschnitt angezeigt. \n",
    "\n",
    "Nur in Visual Studio Code: Nach dem Starten von Visual Studio Code muss zunächst ein Python Interpreter ausgewählt werden. Beim Versuch einen Code-Block auszuführen öffnet sich ein Auswahlfenster, in dem Sie am besten die aktuellste Python-Version auswählen.\n",
    "\n",
    "Falls Änderungen im Code vorgenommen werden, muss nicht der gesamte Code neu ausgeführt werden, sondern nur von der Änderung abhängige Abschnitte. \n",
    "\n",
    "In diesem Tutorial trainieren Sie ein Neuronales Netz darauf , die Handgesten für Schere, Stein und Papier zu erkennen. Der Datensatz enthält 2.892 Bilder von verschiedenen Händen in Stein-Papier-Schere-Posen. Alle Bilder wurden mit Hilfe von CGI-Techniken erzeugt, was den Vorteil bietet, dass keine realen Fotos aufgenommen werden mussten. Jedes Bild ist 300×300 Pixel groß und hat 24-Bit-Farbe.\n",
    "\n",
    "Am Ende des Skriptes finden sich ein paar Aufgaben, deren Bearbeitung Ihr Verständnis zur Thematik fördern soll.\n",
    "\n",
    "Große Teile dieses Skriptes basieren auf einem Tutorial von:\n",
    "https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/rock_paper_scissors_cnn/rock_paper_scissors_cnn.ipynb#scrollTo=k2zZOCweLt1-\n",
    "\n",
    "Der verwendete Datensatz ist hier zu finden: https://www.kaggle.com/datasets/sanikamal/rock-paper-scissors-dataset "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst werden die erforderlichen Bibliotheken importiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import platform\n",
    "import os\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "\n",
    "print('Python version:', platform.python_version())\n",
    "print('Tensorflow version:', tf.__version__)\n",
    "print('Keras version:', tf.keras.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Laden des Datensets\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die TensorFlow Bibliothek stellt einige Datensätze für Tutorials zur Verfuegung, unter anderem den Datensatz mit Bildern der Gesten Schere, Stein, Papier. \n",
    "Damit sich der Workflow in diesem Tutorial und die Programmierung am Präsenztermin ähneln, wird der Datensatz aus einem git-reposotory heruntergeladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Herunterladen des Datensatzes\n",
    "\n",
    "URL = \"https://seafile.projekt.uni-hannover.de/f/0a3bf5ffb2fc44f1b687/?dl=1\"\n",
    "response = requests.get(URL)\n",
    "open(\"Rock_Paper_Scissors_Dataset.zip\", \"wb\").write(response.content)\n",
    "\n",
    "zip = ZipFile('Rock_Paper_Scissors_Dataset.zip')\n",
    "zip.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden des Datensatzes\n",
    "Directory = os.path.abspath(os.getcwd())+\"/Rock-Paper-Scissors_Keras_Dataset\"\n",
    "\n",
    "Split=0.2\n",
    "Default_Image_Size = (300,300)      # Höhe, Breite\n",
    "INPUT_IMG_SHAPE=(300,300,3)        # Höhe, Breite, Farbkanäle\n",
    "\n",
    "dataset_train_raw = tf.keras.utils.image_dataset_from_directory(\n",
    "    Directory, \n",
    "    labels='inferred', \n",
    "    label_mode='int',\n",
    "    class_names=None, \n",
    "    color_mode='rgb', \n",
    "    batch_size = 32, \n",
    "    image_size=Default_Image_Size, \n",
    "    shuffle=True, \n",
    "    seed=1, \n",
    "    validation_split=Split, \n",
    "    subset='training',\n",
    "    interpolation='bilinear', \n",
    "    ) \n",
    "# bis hier\n",
    "\n",
    "dataset_test_raw = tf.keras.utils.image_dataset_from_directory(\n",
    "    Directory, \n",
    "    labels='inferred', \n",
    "    label_mode='int',\n",
    "    class_names=None, \n",
    "    color_mode='rgb', \n",
    "    batch_size = 32, \n",
    "    image_size=Default_Image_Size, \n",
    "    shuffle=True, \n",
    "    seed=1, \n",
    "    validation_split=Split, \n",
    "    subset='validation',\n",
    "    interpolation='bilinear', \n",
    "    )\n",
    "\n",
    "class_names = dataset_train_raw.class_names\n",
    "\n",
    "NUM_TRAIN_EXAMPLES = len(dataset_train_raw.file_paths)\n",
    "NUM_TEST_EXAMPLES = len(dataset_test_raw.file_paths)\n",
    "NUM_CLASSES=len(class_names)\n",
    "\n",
    "print('Label 1: ',class_names[0])\n",
    "print('Label 2: ',class_names[1])\n",
    "print('Label 3: ',class_names[2])\n",
    "\n",
    "dataset_train_raw = dataset_train_raw.unbatch()\n",
    "dataset_test_raw = dataset_test_raw.unbatch() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.system(\"git clone {}\".format('https://gitlab.uni-hannover.de/robin-menzel/Programming_MLL/Rock-Paper-Scissors_Keras_Dataset.git')) \n",
    "\n",
    "#os.system(\"git clone {}\".format('https://seafile.cloud.uni-hannover.de/d/f56e56a8e77c413e8434/')) \n",
    "os.system('https://seafile.cloud.uni-hannover.de/f/a41617b0b991446c8ba4/?dl=1')\n",
    "\n",
    "\n",
    "#os.system(\"git clone {}\".format('https://github.com/match-PM/Masterlabor-MLL')) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Festlegen der Bildgröße und Image-Augmentaion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Festlegen der Bildgröße und Erzeugung eines Datensatzes mit verringerter Bildgröße"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um den Rechenaufwand für das NN zu verringern werden Bilder eines Datensatzes i.d.R. in ihrer Pixel-Größe reduziert. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_IMG_SHAPE_REDUCED = (\n",
    "    INPUT_IMG_SHAPE[0]//2,\n",
    "    INPUT_IMG_SHAPE[1]//2,\n",
    "    INPUT_IMG_SHAPE[2]\n",
    ")\n",
    "\n",
    "print('Input image shape (original):', INPUT_IMG_SHAPE)\n",
    "print('Input image shape (reduced):', INPUT_IMG_SHAPE_REDUCED)\n",
    "\n",
    "#Diese Funktion formatiert die Bildgroeße eines Bildes von der Urspruenglichen in INPUT_IMG_SIZE\n",
    "def format_example(image, label):\n",
    "    # Konvertiert Bildfarbwerte zu float\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # Konvertiert Bildfarbwerte zu Werten zwischen [0,1]\n",
    "    image = image / 255.\n",
    "    # Konvertiert Bildgroesse zu [INPUT_IMG_SIZE, INPUT_IMG_SIZE]\n",
    "    image = tf.image.resize(image, [INPUT_IMG_SHAPE_REDUCED[0], INPUT_IMG_SHAPE_REDUCED[1]])\n",
    "    return image, label\n",
    "\n",
    "# Mit der map-Funktion wird eine Funktion auf ein Datenset angewendet \n",
    "dataset_train = dataset_train_raw.map(format_example)\n",
    "dataset_test = dataset_test_raw.map(format_example)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Definition von Image-Augmentation Funktionen und Erzeugung eines Augmented Datensatzes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch den Einsatz von Imgage-Augmentaion können die Bilder eines Datensatzes für das Training manipuliert werden. Hierdurch sieht das Netz beim Training immer eine Variation des Originalbildes, wodruch der Effekt von Overfitting reduziert werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def augment_color(image):   # Verändert die Sätigung, die Helligkeit, den Kontrast und den Farbton eines Bildes zufällig\n",
    "    image = tf.image.random_hue(image, max_delta=0.08)\n",
    "    image = tf.image.random_saturation(image, lower=0.7, upper=1.3)\n",
    "    image = tf.image.random_brightness(image, 0.05)\n",
    "    image = tf.image.random_contrast(image, lower=0.8, upper=1)\n",
    "    image = tf.clip_by_value(image, clip_value_min=0, clip_value_max=1)\n",
    "    return image\n",
    "\n",
    "def augment_rotation(image): # Dreht das Bild zufällig in einem Bereich von 0-90°\n",
    "    # Rotate 0, 90\n",
    "    return tf.image.rot90(\n",
    "        image,\n",
    "        tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)\n",
    "    )\n",
    "\n",
    "def augment_inversion(image):   # Invertiert ein Bild nach zufälliger Auswahl\n",
    "    random = tf.random.uniform(shape=[], minval=0, maxval=1)\n",
    "    if random > 0.5:\n",
    "        image = tf.math.multiply(image, -1)\n",
    "        image = tf.math.add(image, 1)\n",
    "    return image\n",
    "\n",
    "def augment_zoom(image, min_zoom=0.8, max_zoom=1.0):    # Zoomt ein Bild zufällig heran\n",
    "    image_width, image_height, image_colors = image.shape\n",
    "    crop_size = (image_width, image_height)\n",
    "\n",
    "    # Generiert Beschnitteinstellungen von 1 % bis 20 % \n",
    "    scales = list(np.arange(min_zoom, max_zoom, 0.01))\n",
    "    boxes = np.zeros((len(scales), 4))\n",
    "\n",
    "    for i, scale in enumerate(scales):\n",
    "        x1 = y1 = 0.5 - (0.5 * scale)\n",
    "        x2 = y2 = 0.5 + (0.5 * scale)\n",
    "        boxes[i] = [x1, y1, x2, y2]\n",
    "\n",
    "    def random_crop(img):\n",
    "        # Generiert eine zufällige Beschnitteinstellungen für ein Bild\n",
    "        crops = tf.image.crop_and_resize(\n",
    "            [img],\n",
    "            boxes=boxes,\n",
    "            box_indices=np.zeros(len(scales)),\n",
    "            crop_size=crop_size\n",
    "        )\n",
    "        # Return zufällige Beschnitteinstellung\n",
    "        return crops[tf.random.uniform(shape=[], minval=0, maxval=len(scales), dtype=tf.int32)]\n",
    "\n",
    "    choice = tf.random.uniform(shape=[], minval=0., maxval=1., dtype=tf.float32)\n",
    "\n",
    "    # Zufällig, nur 50% der Fälle\n",
    "    return tf.cond(choice < 0.5, lambda: image, lambda: random_crop(image))\n",
    "\n",
    "# Fasst alle Augment-Funktionen in einer Funktion Zusammen\n",
    "def augment_data(image, label):\n",
    "    image = augment_zoom(image)\n",
    "    image = augment_color(image)\n",
    "    image = augment_rotation(image)\n",
    "    image = augment_inversion(image)\n",
    "    return image, label\n",
    "\n",
    "# Wendet die Augment-Funktion auf das Datenset an\n",
    "\n",
    "dataset_train_augmented = dataset_train.map(augment_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Vergleiche Datensatz mit reduzierter Größe und Augmented-Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mit dieser Funktion können die ersten 12 Bilder eines Datzensatzes in einem 3x4 Plot dargestellt werden - wird in Abschnitt 3 genutzt\n",
    "def preview_dataset(dataset):               \n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plot_index = 0\n",
    "    for features in dataset.take(12):\n",
    "        (image, label) = features\n",
    "        plot_index += 1\n",
    "        plt.subplot(3, 4, plot_index)\n",
    "        label = class_names[label]\n",
    "        plt.title('Label:' + label)\n",
    "        plt.imshow(image.numpy())\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "# So sehen die ersten 12 Bilder des dataset_train Datensatzes aus\n",
    "preview_dataset(dataset_train)\n",
    "\n",
    "# So sehen die ersten 12 Bilder des dataset_train_augmented Datensatzes aus\n",
    "preview_dataset(dataset_train_augmented)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Mischen der Bilder im Datensatz und Unterteilung in Batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 30 # Hier wird die Batchgröße festgelegt    #Default 30\n",
    "\n",
    "dataset_train_augmented_shuffled = dataset_train_augmented.shuffle(# Shuffled den augmentierteb Trainingsdatensatz und speichert es in einer neuen Datenssatzvariable\n",
    "    buffer_size=NUM_TRAIN_EXAMPLES,seed=1\n",
    ")\n",
    "\n",
    "# Konvertiert den Trainingsdatensatz (augmentiert und geshuffelt) in ein Batched Datensatz\n",
    "dataset_train_augmented_shuffled = dataset_train_augmented_shuffled.batch(\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Prefetch erlabut der Input Pipeline Datenbaches bereits während des Trainings zu laden\n",
    "dataset_train_augmented_shuffled = dataset_train_augmented_shuffled.prefetch(\n",
    "    buffer_size=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "\n",
    "# Batched den Testdatensatz\n",
    "dataset_test_shuffled = dataset_test.batch(BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Erstellung eines sequenziellen Modells"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelle können in Keras als sequenzielle (sequential) und funktionale (functional) Modelle erstellt werden. \n",
    "Für Einsteiger sind sequenzielle Modelle einfacher zu implementieren.\n",
    "\n",
    "In diesem Abschnitt wird ein sequenzielles Modell aufgebaut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellt ein neues Sequenzielles Modell mit dem Variablenname \"model\"\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Fügt ein Convolution-Layer (1.Layer) hinzu\n",
    "model.add(tf.keras.layers.Convolution2D(\n",
    "    input_shape=INPUT_IMG_SHAPE_REDUCED,\n",
    "    filters=64,                                 # Anzahl zu trainierender Filter/ Anzahl zu erzeugender Feature Maps\n",
    "    kernel_size=3,                              # Größe der Faltungsmatrizen\n",
    "    activation=tf.keras.activations.relu        # Aktivierungsfunktion der trainierten Filter\n",
    "))\n",
    "\n",
    "# Fügt ein MaxPooling-Layer hinzu\n",
    "model.add(tf.keras.layers.MaxPooling2D(\n",
    "    pool_size=(2, 2),                           # Anzahl an Pixeln für Pooling\n",
    "    strides=(2, 2)                              # Abstand für das Pooling\n",
    "))\n",
    "\n",
    "# Fügt ein Convolution-Layer hinzu\n",
    "model.add(tf.keras.layers.Convolution2D(\n",
    "    filters=64,\n",
    "    kernel_size=3,\n",
    "    activation=tf.keras.activations.relu\n",
    "))\n",
    "\n",
    "# Fügt ein MaxPooling-Layer hinzu\n",
    "model.add(tf.keras.layers.MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    strides=(2, 2)\n",
    "))\n",
    "\n",
    "# Fügt ein Convolution-Layer hinzu\n",
    "model.add(tf.keras.layers.Convolution2D(\n",
    "    filters=128,\n",
    "    kernel_size=3,\n",
    "    activation=tf.keras.activations.relu\n",
    "))\n",
    "# Fügt ein MaxPooling-Layer hinzu\n",
    "model.add(tf.keras.layers.MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    strides=(2, 2)\n",
    "))\n",
    "\n",
    "# Fügt ein Convolution-Layer hinzu\n",
    "model.add(tf.keras.layers.Convolution2D(\n",
    "    filters=128,\n",
    "    kernel_size=3,\n",
    "    activation=tf.keras.activations.relu\n",
    "))\n",
    "\n",
    "# Fügt ein MaxPooling-Layer hinzu\n",
    "model.add(tf.keras.layers.MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    strides=(2, 2)\n",
    "))\n",
    "\n",
    "# Glätten (Flatten) der Netzstruckur für das Dense-Layer\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# Fügt ein Dropout-Layer hinzu\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "# Fügt ein Dense-Layer mit 512 Neuronen und Relu-Aktivierungsfunktion hinzu\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    units=512,                              #Anzahl an Neuronen     \n",
    "    activation=tf.keras.activations.relu\n",
    "))\n",
    "\n",
    "# Output-Layer mit Softmax-Aktivierungsfunktion \n",
    "model.add(tf.keras.layers.Dense(\n",
    "    units=NUM_CLASSES,                      #Anzahl an Neuronen\n",
    "    activation=tf.keras.activations.softmax\n",
    "))\n",
    "\n",
    "# Mit diesem Befehl kann die Struktur eines Modelles ausgegeben werden. Am Ende der Tabelle wird angegeben, wie viele Parameter das erzeugte Netz aufweist und wie viele davon trainierbar bzw. festgelegt sind.\n",
    "# In diesem Fall müssen alle Parameter trainiert werden. \n",
    "# Mithilfe der Spalte \"Output Shape\" kann nachvollzogen werden, welche Auswirkungen das hinzufügen eines Layers auf die Anzahl der Parameter hat. Zur Erinnerung: Unser Input hat Shape (150, 150,3)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Modell kompilieren"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Abschnitt werden die Einstellungen für das Training des Neuronalen Netzes, wie die zu verwendende Fehlergütefunktional, der Optimierungsalgorithmus und die Lernrate festgelegt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)            # Alternativ kann auch der Adam Optimizer genutzt werden\n",
    "rmsprop_optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=rmsprop_optimizer,\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "steps_per_epoch = NUM_TRAIN_EXAMPLES // BATCH_SIZE\n",
    "validation_steps = NUM_TEST_EXAMPLES // BATCH_SIZE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Modell Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Training\n",
    "\n",
    "Mit der Funktion model.fit() wird das Training des Neuronalen Netzes auf den Trainingsdatensatz gestartet.\n",
    "Dies kann mehrere Minuten dauern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoches=3 # Festlegen der Trainingsepochen \n",
    "\n",
    "training_history = model.fit(\n",
    "    x=dataset_train_augmented_shuffled.repeat(),        # Festlegen des Trainingsdatensets\n",
    "    validation_data=dataset_test_shuffled.repeat(),     # Festlegen des Validationdatensets\n",
    "    epochs=train_epoches,\n",
    "    steps_per_epoch=steps_per_epoch,                    # Muss nur definiert werden, wenn dataset.repeat() in fit() verwendet wird, damit der Trainingsalgorithmus eine Stop Condition hat.\n",
    "    validation_steps=validation_steps,                  # Muss nur definiert werden, wenn dataset.repeat() in fit() verwendet wird\n",
    "    verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2  Trainingshistorie grafisch abbilden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diese Funktion stellt die Trainings-Historie grafisch dar\n",
    "def render_training_history(training_history):\n",
    "    loss = training_history.history['loss']\n",
    "    val_loss = training_history.history['val_loss']\n",
    "\n",
    "    accuracy = training_history.history['accuracy']\n",
    "    val_accuracy = training_history.history['val_accuracy']\n",
    "\n",
    "    plt.figure(figsize=(14, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(loss, label='Training set')\n",
    "    plt.plot(val_loss, label='Test set', linestyle='--')\n",
    "    plt.legend()\n",
    "    plt.grid(linestyle='--', linewidth=1, alpha=0.5)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.plot(accuracy, label='Training set')\n",
    "    plt.plot(val_accuracy, label='Test set', linestyle='--')\n",
    "    plt.legend()\n",
    "    plt.grid(linestyle='--', linewidth=1, alpha=0.5)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Grafische Darstellung der Trainings-Historie\n",
    "render_training_history(training_history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Speichern und Laden von Modellen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So kann ein trainiertes NN gespeichert und geladen werden. Dies ist z.B. sinnvoll falls das trainierte Netz in einer Anwendung genutzt werden soll. Z.B. können Sie durch das Laden Ihres Netzes in einer späteren Session die Live-Kameravorhersage erneut dürchführen, ohne erneut ein Netz trainieren zu müssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'My_Model_R_P_S.h5'\n",
    "# Modell wird unter dem Namen \"model_name\" gespeichert\n",
    "model.save(model_name, save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Läd das Modell mit dem Namen \"model_name\" und weist es der Variablen \"model\" zu\n",
    "model = tf.keras.models.load_model('My_Model_R_P_S.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Klassifizierungsvorhersagen aus Bildern des Test_Datensatzes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der Funktion predict() kann das NN für Vorhersagen genutzt werden. \n",
    "\n",
    "In diesem Abschnitt  werden für die ersten 12 Bilder des 1. Batches des Testdatensatzes vorhersagen gemacht. \n",
    "\n",
    "Die Bilder, sowie die Klassenvorhersage und das tatsächliche Label werden dargestellt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, label in dataset_test_shuffled.take(3): # Take 1 Batch, Bei 3 -> Take 3 Batches\n",
    "  prediction=model.predict(images)                  # Matrix des Größe (Batch_Size x Anz. der Klassen) gefüllt mit Konfidenzwerten \n",
    "  plt.figure(figsize=(15,10))\n",
    "\n",
    "  for i in range(12):                               # Ersten 12 Bilder eines Batches\n",
    "    predicted_class=np.argmax(prediction[i])        # Welche Klasse hat den höchsten Konfidenzwert\n",
    "    plt.subplot(3, 4, i + 1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title('P: '+ class_names[predicted_class]+ \" L: \"+class_names[label[i]], fontsize=15)  # P = Prediction, L = Label from Picture\n",
    "    plt.imshow(images[i])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Vorhersagen aus Live-Kamerabild"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das NN kann auch genutzt werden um Live-Vorhersagen aus dem Kamerabild zu machen. \n",
    "Damit dieser Codeabschnitt funktioniert brauchen Sie entwieder eine integrierte Laptopkamera oder besser, eine externe Webcam.\n",
    "\n",
    "Nach dem Ausführen wird ein Kamerabild, mit dem vorhergesagten Label und dem Konfidenzwert angezeigt.\n",
    "Mit der Taste 'q' wird das Kamerabild geschlossen.\n",
    "\n",
    "In manchen Fällen hat dieser Teil in der Vergangenheit nicht auf den persönlichen Computern der Teilnehmer funktioniert. Falls dies bei Ihnen der Fall sein sollte, können Sie diesen Teil auch über das Python File \"Predict with Camera.py\" aufrufen. Achten Sie darauf, dass sich der Code in demselben Ordner befinden muss, wie das gespeicherte Model \"My_Model_R_P_S.h5\".\n",
    "\n",
    "Sollte das Python File ebenfalls nicht funktionieren, wenden Sie sich an den Betreuer der Veranstaltung. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "\n",
    "IMAGE_HIGHT=INPUT_IMG_SHAPE_REDUCED[0]\n",
    "IMAGE_WIDTH=INPUT_IMG_SHAPE_REDUCED[1]\n",
    "\n",
    "Threshold = 0.7\n",
    "\n",
    "def FormatingDataset(images):\n",
    "  images = tf.cast(images, tf.float32)\n",
    "  images = images / 255.\n",
    "  images = tf.image.resize(images, (IMAGE_HIGHT, IMAGE_WIDTH))\n",
    "  return images\n",
    "\n",
    "VideoFeed = cv2.VideoCapture(0)                        # Falls kein VideoFeed angezeigt wird, die 0 durch 1, 2, etc. ersetzen\n",
    "width = int(VideoFeed.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(VideoFeed.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "while VideoFeed.isOpened(): \n",
    "    ret, frame = VideoFeed.read()\n",
    "    images = FormatingDataset(frame)\n",
    "    img_pred=(np.expand_dims(images,axis=0))\n",
    "\n",
    "    prediction=model.predict(img_pred)\n",
    "    predicted_class=np.argmax(prediction)\n",
    "    if (prediction[0][predicted_class] > Threshold):\n",
    "        text=str(class_names[predicted_class] + \"  Value: \" + str(prediction[0][predicted_class]))\n",
    "        cv2.putText(frame,text,(int(0),int(50)),cv2.FONT_HERSHEY_TRIPLEX, 0.8, (0,255,0),1)\n",
    "    cv2.imshow('Stream',frame)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        VideoFeed.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Aufgaben"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wichtiger Hinweis für die Aufgaben: Nach Änderungen an Datensätzen und Trainingsparameter muss das Modell (Abschnitt 5 + Abschnitt 6) neu geladen werden. Nach dem Training werden die Parameter eines Modells gespeichert, sodass bei erneutem Training das Optimierungsproblem einen anderen Ausgangspunkt hat. Z.B. kann das Modell bereits bestmöglich angepasst sein, wodurch sich die Werte in der Trainingshistorie evtl. nicht mehr sichtbar verbessern."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Variieren Sie die Batch_Size, die Epochenzahl und die Lernrate wie unten angeben und trainieren Sie das Netz. Betrachten Sie die Trainingshistorie sowie die Rechenzeit. Was können Sie aus den Versuchen für den Einfluss dieser Parameter ableiten? Hinweis: Die Rechenzeit wird unten links im Code-Block angezeigt. Initialisieren Sie das Model nach jeden Schritt neu (Abschnitt 5 + Abschnitt 6).\n",
    "\n",
    "    Batch-Size = 5 -              Epochen = 15 -         Lernrate = 0.001\n",
    "\n",
    "    Batch-Size = 30 -             Epochen = 15 -         Lernrate = 0.001\n",
    "\n",
    "    Batch-Size = 100 -            Epochen = 15  -        Lernrate = 0.001\n",
    "\n",
    "    Die Plots der Trainingshistorien der folgenden Versuche wurden Ihnen bei Stud.IP bereitgestellt und müssen deshalb nicht von Ihnen erzeugt werden. \n",
    "\n",
    "    Batch-Size = 30 -             Epochen = 15 -         Lernrate = 0.01\n",
    "\n",
    "    Batch-Size = 30 -             Epochen = 5 -          Lernrate = 0.001\n",
    "\n",
    "    Batch-Size = 30 -             Epochen = 50 -         Lernrate = 0.001\n",
    "    \n",
    "\n",
    "Setzen Sie die Parameter für die folgenden Aufgaben auf Batch-Size=30, Epochen=15 und Lernrate=0.001."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Vergleiche welchen Einfluss die Image-Augmentation auf das Trainingsergebnis hat. Trainiere das Netz mit und ohne Image Augmentation und Vergleiche Validation_Loss und Validation_accuracy (der Graf mit Image-Augmantaion kann auch aus Aufgabe 1 entnommen werden). Stellen Sie dafür die grafischen Darstellungen der Trainingshistorie gegenüber. Was leiten Sie als Nutzen der Image Augmentation ab?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Überlegen Sie, welche Image-Augmentation Funktionen sinnvoll sind und welche nicht? Fügen Sie eine Funktion zur Image-Augmentation hinzu, welche die Bilder des Datensatzes zufällig entlang der horizontalen und vertikalen Achse spiegelt.\n",
    "Dies ist mit den folgenden Funktionen möglich:\n",
    "\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    \n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "\n",
    "Trainieren Sie Ihr Netz anschließend mit Batch-Size=30, Epochen=20 und Lernrate=0.001."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Vergleiche, wie die Trainingshistorie mit einer Lernrate von 0.0001 (Epochen=10) verläuft. Initialisieren Sie das Netz in Abschnitt 5 dafür NICHT!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speichern Sie das akutelle Modell nachdem Sie Aufgabe 3 und 4 absolviert haben."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Starte die Vorhersage aus dem Live-Kamerabild. Lade zuvor das Modell, welches Sie nach Aufgabe 4 gespeichert haben. \n",
    "\n",
    "    Funktioniert die Live-Vorhersage wie von Ihnen erwartet? \n",
    "    Richten Sie die Kamera so aus, dass das Kamerabild den Trainingsbildern möglichst gut ähnelt und probieren Sie, ob die von Ihnen durchgeführten Gesten korrekt klassifiziert werden (Auch ein schwarzer Untergrund (bzw. ein farbiger Untergrund) funktionieren)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f701b6929a839fa417ceb7454ef4bfcbf51ba2477385a82ceb157f8da4252d35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
